{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13b2a81e-2561-4f78-8b4b-650301b48d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/kaiyuep/.local/lib/python3.12/site-packages (4.46.1)\n",
      "Requirement already satisfied: filelock in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/kaiyuep/.local/lib/python3.12/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/kaiyuep/.local/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/kaiyuep/.local/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/kaiyuep/.local/lib/python3.12/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/kaiyuep/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/kaiyuep/.local/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/kaiyuep/.local/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/kaiyuep/.local/lib/python3.12/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /soft/applications/miniconda3/conda_pytorch/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install pandas\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca412a5a-cf9a-42fd-a12f-ed47e9d16239",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "import numpy \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "training_data = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "\n",
    "train_size = int(0.8 * len(training_data))  # 80% for training\n",
    "val_size = len(training_data) - train_size  # Remaining 20% for validation\n",
    "training_data, validation_data = torch.utils.data.random_split(training_data, [train_size, val_size], generator=torch.Generator().manual_seed(55))\n",
    "\n",
    "class NonlinearClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "           # nn.Dropout(0.2),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "           # nn.Dropout(0.2),\n",
    "            nn.Linear(50, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.layers_stack(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_one_epoch(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # backward pass calculates gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # take one step with these gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # resets the gradients \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "def evaluate(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - some NN pieces behave differently during training\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    loss, correct = 0, 0\n",
    "\n",
    "    # We can save computation and memory by not calculating gradients here - we aren't optimizing \n",
    "    with torch.no_grad():\n",
    "        # loop over all of the batches\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, y).item()\n",
    "            # how many are correct in this batch? Tracking for accuracy \n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    accuracy = 100*correct\n",
    "    return accuracy, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04810ed4-d074-4464-a649-937edbeecbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##################batch size###########################\n",
    "batch_size_ls = [32,64,128,256,521]\n",
    "epochs = 5\n",
    "\n",
    "for i in range (len(batch_size_ls)):\n",
    "    nonlinear_model = NonlinearClassifier()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(nonlinear_model.parameters(), lr=0.05)\n",
    "    train_acc_all = []\n",
    "    val_acc_all = []\n",
    "    batch_size=batch_size_ls[i]\n",
    "    print('batch_size',batch_size)\n",
    "    train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\n",
    "    val_dataloader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)\n",
    "    for j in range(epochs):\n",
    "        train_one_epoch(train_dataloader, nonlinear_model, loss_fn, optimizer)\n",
    "        \n",
    "        # checking on the training loss and accuracy once per epoch\n",
    "        acc, loss = evaluate(train_dataloader, nonlinear_model, loss_fn)\n",
    "        train_acc_all.append(acc)\n",
    "        print(f\"Epoch {j}: training loss: {loss}, accuracy: {acc}\")\n",
    "        \n",
    "        # checking on the validation loss and accuracy once per epoch\n",
    "        val_acc, val_loss = evaluate(val_dataloader, nonlinear_model, loss_fn)\n",
    "        val_acc_all.append(val_acc)\n",
    "        print(f\"Epoch {j}: val. loss: {val_loss}, val. accuracy: {val_acc}\")\n",
    "\n",
    "    pltsize=1\n",
    "    plt.figure(figsize=(5*pltsize, 5 * pltsize))\n",
    "    plt.plot(range(epochs), train_acc_all,label = 'Training Acc.' )\n",
    "    plt.plot(range(epochs), val_acc_all, label = 'Validation Acc.' )\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee73a1f4-4021-42ed-897a-daac9ecf1fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##################lr###########################\n",
    "lr_ls = [0.01,0.05,0.1,0.5]\n",
    "epochs = 5\n",
    "batch_size=64\n",
    "for i in range (len(lr_ls)):\n",
    "    lr=lr_ls[i]\n",
    "    nonlinear_model = NonlinearClassifier()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(nonlinear_model.parameters(), lr=lr)\n",
    "    train_acc_all = []\n",
    "    val_acc_all = []\n",
    "    print('batch_size',batch_size)\n",
    "    train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\n",
    "    val_dataloader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)\n",
    "    for j in range(epochs):\n",
    "        train_one_epoch(train_dataloader, nonlinear_model, loss_fn, optimizer)\n",
    "        \n",
    "        # checking on the training loss and accuracy once per epoch\n",
    "        acc, loss = evaluate(train_dataloader, nonlinear_model, loss_fn)\n",
    "        train_acc_all.append(acc)\n",
    "        print(f\"Epoch {j}: training loss: {loss}, accuracy: {acc}\")\n",
    "        \n",
    "        # checking on the validation loss and accuracy once per epoch\n",
    "        val_acc, val_loss = evaluate(val_dataloader, nonlinear_model, loss_fn)\n",
    "        val_acc_all.append(val_acc)\n",
    "        print(f\"Epoch {j}: val. loss: {val_loss}, val. accuracy: {val_acc}\")\n",
    "\n",
    "    pltsize=1\n",
    "    plt.figure(figsize=(5*pltsize, 5 * pltsize))\n",
    "    plt.plot(range(epochs), train_acc_all,label = 'Training Acc.' )\n",
    "    plt.plot(range(epochs), val_acc_all, label = 'Validation Acc.' )\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3045a94f-4263-4f18-9bce-cd3e060b96b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (pytorch)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
